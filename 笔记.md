## 线性回归

    读取数据文本csv文件的函数包为    github.com/go-gota/gota/dataframe    
    直观显示(画图)的函数包为       gonum.org/v1/plot
    线性回归用到的函数包为         github.com/sajari/regression
    首先分析数据，从一些统计性上， 在dataframe包下，有Describe()看一些
    最大值、最小值、平均值、标准差等标准，可以用于后续的模型检测
    
    线性回归的假设和陷阱
    需要知道，机器学习不能覆盖所有的场景、线性回归作为其中一员更是，而且，线性回归是提前假定
    因变量是随自变量线性相关的，但实际复杂的情况，并不完全是这样的，但同时，它也是最简单的假设，
    容易理解和可视化。

    线性回归的假设：
    1、线性关系：这可能是显而易见的，但是线性回归假设因变量线性依赖（通过线性方程）自变量。 
    如果这种关系是非线性的，线性回归的性能可能表现不佳。
    2、正态性：这个假设意味着变量是正态分布的（看起来像钟形）。
    3、非多重共线性：多重共线性是一个奇特的术语，它意味着自变量并不是真正独立，它们以某种方式相互依赖。
    4、无自相关性：自相关是另一个奇特的术语，它意味着变量依赖于自身或者自身的一些历史版本（例如一些可预测的时间序列）。
    5、同方差性：这可能是一系列术语中最奇特的词语，但是它意味着相对简单、并不需要经常担心的东西。
    线性回归假设自变量的所有取值在回归线附近的方差基本相同。
    从技术上讲，使用线性回归时所有这些假设应该满足。了解数据的分布和行为是非常重要的。
    当使用线性回归分析示例中的数据时，将会研究这些假设。

    线性回归的陷阱：
    1、你在为某个特定范围的自变量训练线性回归模型时。对这个范围以外的数值预测应该非常小心，
    因为回归线可能并不适用于这种情况（例如，因变量在极值处可能开始表现为非线性）。
    2、可能因为找到两个原本没有关系的变量之间的虚假关系，从而错误地采用线性回归模型。
    应当仔细检查以确保变量功能上相关是有内在逻辑原因的。
    3、在某些拟合中，数据中的异常或极值可能偏离回归线，例如OLS。有多种方法可以拟合对异常免疫的回归线，
    或者对异常敏感的回归线，例如最小正交二乘或者岭回归。
    
    岭回归 ridge regression 实现的函数包 github.com/berkmancenter/ridge

    为了避免过度拟合，应该将数据分为训练数据、测试数据、保留数据。这种方式通用于机器学习。

    书中的例子采用了循序渐进的方式，因为从数据的各自属性上看，一开始只有TV和Sales大概符合线性。
    所以采用1对1的方式，建立回归方程。从MAE这个指标为目标分析，逐步加入的Radio作为自变量。
    加入后，改善了MAE，说明添加是正确的。
    这里需要说明的是，当为模型添加更多的复杂性时(自变量数量)。牺牲了简单性，并且可能会有过度拟合的危险。
    所以只有模型性能能提升能够创造更多价值的时候，才会增加模型复杂性。

## 逻辑回归

    逻辑回归的假设：
    1、比值的对数呈线性关系：逻辑回归潜在的假设是可以构建比值比对数的线性模型。
    2、因变量的编码：在前面创建的模型中，假设尝试预测B的概率，其中1.0的概率对应示例B。
    因此也需要按这种编码类型准备数据。
    3、观察的独立性：数据中每个x示例都应该保持独立，也就是说，要避免相同示例被包含多次。
    
    逻辑回归的陷阱：
    1、与其他分类技术相比，逻辑回归对异常值更加敏感。应留心这点并尝试相应地分析数据。
    2、由于逻辑回归所依赖的指数函数从未真正达到0.0或者1.0（除了+/-无穷大以外），因此评估指标不太可能退化。
    
    逻辑回归是保持了可解释性的非常强大的方法，它是一个灵活的模型，在解决分类问题时应当首先考虑。

## kNN

    将原始数据传入叫instances的内部格式  github.com/sjwhitworth/golearn/base
    对模型进行交叉验证的函数包   github.com/sjwhitworth/golearn/evaluation
    实现kNN的函数包   github.com/sjwhitworth/golearn/knn
    kNN的假设和陷阱：
    1、kNN属于惰性评估。这意味着只有当需要预测时，才计算距离或相似度。在预测之前，并不训练或匹配任何东西。
    这样做有它的优势，但是当有很多数据点时，计算和搜索点时可能会很慢。
    2、k是可以随意选择的，但在选择k时也有一些形式化的方法，用以合理选择k。选择k的常用技术就是搜索k值的范围。
    例如，可以从k=2开始。然后，可以开始增加k，并且对于每个k，在一个测试集上进行评估。
    3、kNN没有考虑哪些特征相对于其他特征更重要。而且，如果一个特征的规模比其他特征大得多，
    这可能会不自然地加重这些较大特征的重要性。

##决策树和随机森林
    
    决策树的函数包 github.com/sjwhitworth/golearn/trees
    随机森林的函数包    github.com/sjwhitworth/golearn/ensemble
    有多种方法可以选择决策树的构建方式、拆分方式等。确定决策树如何构建的最常见方法之一是使用称为熵（entropy）的量。
    基本上，创建和分割树是基于那些可以为待解问题提供最多信息的特征。特征越重要，在树上的优先级就越高。
    重要特征的优先级和清晰的结构使得决策树非常易于理解。也使得决策树对于那些需要解释推断（例如合规性要求）的应用来说非常重要。
    但是，对于训练数据的变化，单个决策树可能不稳定。换句话说，即使训练数据发生微小变化，树的结构也可能发生显著变化。
    这会在运作管理和认知管理方面 都具有挑战性，这也是随机森林模型创建的原因之一。
    
    随机森林是决策树的集合，它们共同作出预测。与单一决策树相比，随机森林更加稳定，并且它们对于过度拟合更为稳健。
    事实上，这种将模型整合到一个整体中的想法在整个机器学习中非常普遍，既可以提高简单分类器（如决策树）的性能，又可以防止过度拟合。
    
    为了构造一个随机森林，可以选择特征的 N个随机子集，并基于这些子集构造N个独立的决策树。
    在进行预测时，让这N个决策树中的每一个都进行预测。为了获得最终的预测，可以对这N个预测进行多数投票。

    决策树和随机森林的假设和陷阱：
    1、单一的决策树模型容易过度拟合数据，特别是在不限制树的深度的情况下。
    大多数实现允许通过一个参数来限制这个深度（或者修剪决策树）。
    修剪参数通常会允许删除对预测影响不大的部分树，从而降低模型的总体复杂度。
    2、当开始谈论诸如随机森林之类的集合模型时，我们正在进入会有些不透明的模型。
    很难获得一个模型的整体直觉，必须在某种程度上像黑盒子一样对待它。因此只有在必要时才应用这些较难解释的模型。
    3、虽然决策树本身的计算效率非常高，但随机森林的计算效率可能非常低，具体取决于拥有多少特征以及随机森林中有多少棵树。

    例子中随机森林的正确率没有决策树高的原因是：
    建立随机森林，需要告诉软件包要构建多少棵数以及每棵树有多少个随机选择的特征。
    每棵树特征数量的理想默认值是总特征数的平方根，但是对于小数据集，这种选择并不会产生很好的结果，因为做一个好的
    预测需要用到所有的特征值。

##朴素贝叶斯
    
    朴素贝叶斯的函数包   github.com/sjwhitworth/golearn/naive
    朴素贝叶斯是一种基于概率的方法，就像逻辑回归，但其基本思想和假设是不同的。
    朴素贝叶斯的假设和陷阱：
    1、朴素贝叶斯有一个重要的假设。这个假设表明，类别的概率以及数据集中某个特征的存在与否，与数据集中是否存在其他特征无关。
    因此，只要提供某些特征存在或不存在，就可以为特定类别的概率写出一个非常简单的公式。
    
##集群
    
    机器学习算法，分为有监督的和无监督的。
    有监督的：有一组已知的特征或属性与尝试预测的标签或编号相匹配。利用这些已标记的数据将模型拟合到特定行为上，
    这些行为是在训练模型之前就已知的。
    无监督的：
    k-均值集群的函数包  github.com/mash/gokmeans
    
    k-均值的假设：
    1、球形或空间分组的集群：k－均值基本上是在特征空间中绘制球形或空间上接近的区域，用以找到集群。
    这意味着对于非球形集群（也就是说，集群看起来不像特征空间中的分组区块），k－均值可能会失败。
    2、相似的大小：k－均值也假定集群都是相似的大小。小的外围集群可能导致简单的k－均值算法偏离过程，从而产生奇怪的分组。
    
    k-均值的陷阱：
    1、k可以随意选择。这意味着可以选择一个不合逻辑的k，同时也意味着可以继续增加k，直到对每个点都有一个集群
    （这将是非常好的集群，因为每个点与 它本身完全相同）。为了指导k的选择，应该使用肘形图（elbow graph）方法。
    在这种方法中，计算评估指标时增加k。随着k的增加，评估指标应该越来越好，但最终会出现一个拐点，表明收益递减。
    理想的k就在这个拐点，也称为肘部。
    2、不能保证k－均值总是会收敛到相同的集群。当从随机质心开始时，k均值算法可能在不同的运行中收敛到不同的局部最小值。
    应该意识到这一点，并从各种初始化中运行k-均值算法以确保稳定性。

    